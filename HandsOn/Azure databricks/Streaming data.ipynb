{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908449fb-34b5-428f-8145-19e863939744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/streaming/input/sales_data.csv\") \n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/streaming/input/customer_data.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9f1e2d-40ea-430f-90db-bed190fe44da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: integer (nullable = true)\n |-- OrderDate: string (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: double (nullable = true)\n\nroot\n |-- CustomerID: string (nullable = true)\n |-- CustomerName: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- SignupDate: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "#Initialize SparkSession\n",
    "spark=SparkSession.builder \\\n",
    ".appName(\"StructuredStreamingExample\") \\\n",
    ".getOrCreate()\n",
    "# Define the schema for the CSV data\n",
    "sales_schema= \"OrderID INT, OrderDate STRING, CustomerID STRING, Product STRING, Quantity INT, Price DOUBLE\"\n",
    "#Read streaming data from CSV files\n",
    "df_sales_stream=spark.readStream \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".schema (sales_schema) \\\n",
    ".load(\"dbfs:/FileStore/streaming/input/\")\n",
    "df_sales_stream.printSchema()\n",
    "#Define the schema for the JSON data\n",
    "customer_schema= \"CustomerID STRING, CustomerName STRING, Region STRING, SignupDate STRING\"\n",
    "df_customers_stream=spark.readStream \\\n",
    ".format(\"json\") \\\n",
    ".schema (customer_schema) \\\n",
    ".load(\"dbfs:/FileStore/streaming/input/\")\n",
    "df_customers_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6b265c-664c-4cd8-864e-9abfe11d5543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied transformations on sales data...\nAggregated sales data by product...\nApplied transformations on customer data...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, datediff, to_timestamp\n",
    "#Transform the sales data: Add a new column for total amount\n",
    "df_sales_transformed = df_sales_stream.select(\n",
    "\tcol (\"OrderID\"),\n",
    "\tto_timestamp(col(\"OrderDate\"), \"yyyy-MM-dd HH:mm:ss\").alias (\"OrderDate\"), \n",
    "\tcol (\"Product\"),\n",
    "\tcol(\"Quantity\"),\n",
    "\tcol(\"Price\"),\n",
    "\t(col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")\n",
    ")\n",
    "print(\"Applied transformations on sales data...\")\n",
    "# Add watermark to handle late data and perform an aggregation\n",
    "df_sales_aggregated=df_sales_transformed \\\n",
    "\t.withWatermark(\"OrderDate\", \"1 day\") \\\n",
    "\t.groupBy(\"Product\") \\\n",
    "\t.agg({\"TotalAmount\": \"sum\"})\n",
    "print(\"Aggregated sales data by product...\")\n",
    "#Transform the customer data: Add a new column for the number of years since signup\n",
    "df_customers_transformed = df_customers_stream.withColumn(\n",
    "\t\"YearsSinceSignup\",\n",
    "\tdatediff (current_date(), to_timestamp(col(\"SignupDate\"), \"yyyy-MM-dd\")).cast(\"int\") / 365\n",
    ")\n",
    "print(\"Applied transformations on customer data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399ece49-8f0c-431b-a5c6-80eff322be35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started streaming query to write aggregated sales data to console...\nStarted streaming query to write transformed customer data to console...\n"
     ]
    }
   ],
   "source": [
    "#Write the aggregated sales data to a console sink for debugging\n",
    "sales_query= df_sales_aggregated.writeStream \\\n",
    "\t.outputMode(\"update\") \\\n",
    "\t.format(\"console\") \\\n",
    "\t.start()\n",
    "print(\"Started streaming query to write aggregated sales data to console...\")\n",
    "# Write the transformed customer data to a console sink for debugging\n",
    "customers_query= df_customers_transformed.writeStream \\\n",
    "\t.outputMode(\"append\") \\\n",
    "\t.format(\"console\") \\\n",
    "\t.start()\n",
    "print(\"Started streaming query to write transformed customer data to console...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec286601-333c-42d6-a2ac-94c6bbcadb06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data saved to /dbfs/FileStore/sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Create sample sales data\n",
    "data = {\n",
    "\"OrderID\": [1, 2, 3, 4],\n",
    "\"OrderDate\": [\"2024-01-01 10:00:00\", \"2024-01-02 11:00:00\", \"2024-01-03 12:00:00\", \"2024-01-04 13:00:00\"],\n",
    "\"CustomerID\": [\"c001\", \"c002\", \"C003\", \"C004\"], \"Product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\"], \"Quantity\": [10, 20, 15, 5],\n",
    "\"Price\": [100.0, 200.0, 150.0, 50.0]\n",
    "}\n",
    "#Convert to DataFrame\n",
    "df= pd.DataFrame(data)\n",
    "#Save to CSV\n",
    "csv_path = \"/dbfs/FileStore/sales_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Sample data saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef22dfaf-b785-4bb5-815c-1696423653ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\nData Transformed Successfully\nTransformed data written to Delta table successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured StreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "print(\"Data Loaded Successfully\")\n",
    "\n",
    "# Transform the data: Add a new column for total amount\n",
    "df_transformed = df.withColumn(\n",
    "    \"TotalAmount\",\n",
    "    col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\")\n",
    ")\n",
    "print(\"Data Transformed Successfully\")\n",
    "\n",
    "# Write transformed data to a Delta table with schema evolution enabled\n",
    "df_transformed.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/delta/sales_data\")\n",
    "print(\"Transformed data written to Delta table successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
