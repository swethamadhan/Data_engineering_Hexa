{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "367a71b5-1872-40d0-9cf6-7bf0f1cacdac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create Delta Tables Using 3 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdf075d-6e67-421a-801f-9e9ec88d453a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Move the file from Workspace to DBFS\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/sales_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4212364-211b-40b5-b18b-f827eda87ca6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "# Define the schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"SignupDate\", StringType(), True)  # Keep as StringType to convert later\n",
    "])\n",
    "\n",
    "# Load the JSON with the defined schema\n",
    "customer_df = spark.read.format(\"json\").schema(customer_schema).load(\"/FileStore/customer_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2416dfce-9baa-40c1-8a58-31d403b3ebfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n|2024-09-03| South|Widget B|      20|15.75|\n|2024-09-03|  East|Widget C|      10|22.50|\n|2024-09-04| North|Widget D|       7|30.00|\n|2024-09-04|  West|Widget B|       9|15.75|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV into a DataFrame\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3219328-0cc4-437f-8e3f-42f601e1f56a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the sales data DataFrame as a Delta Table\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d823f42-5f45-4fb7-9564-8a849ae8ca63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move the file to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "\n",
    "# Load the JSON into a DataFrame\n",
    "customer_df = spark.read.format(\"json\").load(\"/FileStore/customer_data.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79efdeab-5522-40fd-aa1f-50afe817e5b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the customer data DataFrame as a Delta Table\n",
    "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8084d82-1b0c-460a-ba1b-c7e8b0e72d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "sample_data = [(\"John\", 30), (\"Jane\", 25), (\"Sam\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "sample_df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "# Write the DataFrame as a Parquet file\n",
    "sample_df.write.mode(\"overwrite\").parquet(\"/FileStore/sample_parquet_file.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d8b8c3-862d-493a-8742-e7f600f4ec21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Parquet file\n",
    "parquet_df = spark.read.parquet(\"/FileStore/sample_parquet_file.parquet\")\n",
    "\n",
    "# Convert the Parquet file to a Delta Table\n",
    "parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sample_parquet_to_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5694b4-7691-47c3-9b3a-5cc58935bf97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f575ff-76de-4fcb-8b50-4140abe527b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966254db-4376-4435-84be-d71dba17a71c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load new_sales_data.csv into DataFrame\n",
    "new_sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/new_sales_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e833a50f-501d-4a9e-a737-dc108b33c94e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the new DataFrame as Delta Table\n",
    "new_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/new_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f42651e-635d-4481-afef-a846c527e865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Load existing Delta table\n",
    "delta_table =spark.read.format(\"delta\").load(\"/delta/sales_data\")\n",
    "delta_new_sales=spark.read.format(\"delta\").load(\"/delta/new_sales_data\")\n",
    "# Create temporary views for SQL operations\n",
    "delta_table.createOrReplaceTempView(\"delta_sales_data\")\n",
    "delta_new_sales.createOrReplaceTempView(\"new_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d008197-5b77-4e5d-99a9-bc77510036d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge new sales data into existing Delta Table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales_data AS target\n",
    "          USING new_sales_data AS source\n",
    "          ON target.OrderID = source.OrderID\n",
    "          WHEN MATCHED THEN UPDATE SET \n",
    "              target.Date = source.OrderDate, \n",
    "              target.CustomerID = source.CustomerID,   \n",
    "              target.Product = source.Product,\n",
    "              target.Quantity = source.Quantity, \n",
    "              target.Price = source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, Customer_ID, Product, Quantity, Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef6ef85-396b-41b1-9cd6-beb26d6b5bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimize Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c34449-58d5-432f-be20-ec13d0d227ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimize the Delta Table and Z-Order by CustomerID\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_sales1_table ZORDER BY CustomerID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "372ccf5d-d7d6-47fc-88fb-faa070e3e1f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9844d7c1-e932-42ca-b2ca-e85b765c88a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+------------+-------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation   |operationParameters                                                            |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                   |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+------------+-------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|12     |2024-09-13 09:00:43|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM END  |{status -> COMPLETED}                                                          |NULL|{3236871288653771}|0911-085840-w2xap01c|11         |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|11     |2024-09-13 09:00:38|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000}           |NULL|{3236871288653771}|0911-085840-w2xap01c|10         |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                   |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|10     |2024-09-13 09:00:15|4164096982719713|azuser2140_mml.local@techademy.com|OPTIMIZE    |{predicate -> [], auto -> false, clusterBy -> [], zOrderBy -> [], batchId -> 0}|NULL|{3236871288653771}|0911-085840-w2xap01c|9          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 1162, p25FileSize -> 1369, numDeletionVectorsRemoved -> 0, minFileSize -> 1369, numAddedFiles -> 1, maxFileSize -> 1369, p75FileSize -> 1369, p50FileSize -> 1369, numAddedBytes -> 1369}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|9      |2024-09-13 09:00:03|4164096982719713|azuser2140_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                   |NULL|{3236871288653771}|0911-085840-w2xap01c|8          |WriteSerializable|false        |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1162}                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|8      |2024-09-13 08:57:51|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM END  |{status -> COMPLETED}                                                          |NULL|{3236871288653771}|0911-085840-w2xap01c|7          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|7      |2024-09-13 08:57:45|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000}           |NULL|{3236871288653771}|0911-085840-w2xap01c|6          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                   |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|6      |2024-09-13 08:34:43|4164096982719713|azuser2140_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                   |NULL|{3236871288653732}|0911-085840-w2xap01c|5          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 9, numOutputBytes -> 1441}                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|5      |2024-09-13 07:24:12|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM END  |{status -> COMPLETED}                                                          |NULL|{683343941130958} |0911-085840-w2xap01c|4          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2024-09-13 07:24:06|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000}           |NULL|{683343941130958} |0911-085840-w2xap01c|3          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                   |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2024-09-13 07:23:42|4164096982719713|azuser2140_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                   |NULL|{683343941130958} |0911-085840-w2xap01c|2          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 9, numOutputBytes -> 1441}                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2024-09-13 07:23:31|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM END  |{status -> COMPLETED}                                                          |NULL|{683343941130958} |0911-085840-w2xap01c|1          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2024-09-13 07:23:25|4164096982719713|azuser2140_mml.local@techademy.com|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000}           |NULL|{683343941130958} |0911-085840-w2xap01c|0          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                   |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2024-09-13 06:31:44|4164096982719713|azuser2140_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                   |NULL|{683343941130958} |0911-085840-w2xap01c|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 9, numOutputBytes -> 1441}                                                                                                                                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+------------+-------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Describe the history of the Delta Table\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/delta/sales_data`\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b786021-926d-4d87-a4b5-a8deab086f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform VACUUM to remove old files from the Delta Table\n",
    "spark.sql(\"VACUUM delta.`/delta/sales_data`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167648e8-ab73-4189-b113-e6cd3c2f021e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hands-on Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea6fdf0-cb8a-4b4d-9665-565c89c2a098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n|2024-09-03| South|Widget B|      20|15.75|\n|2024-09-03|  East|Widget C|      10|22.50|\n|2024-09-04| North|Widget D|       7|30.00|\n|2024-09-04|  West|Widget B|       9|15.75|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Query an older version of the Delta Table using Time Travel\n",
    "sales_df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/sales_data\")\n",
    "sales_df_version_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479541d7-50c9-4c36-b2b9-1c1fca73d4da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John Doe\", 30), (\"Jane Doe\", 25)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(data, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df8d357-77fe-412b-ab1a-93ec837c13c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enforce schema when writing to a Delta Table\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/delta/sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5335b154-ffa1-42eb-839d-fcba017a31c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the Delta Table\n",
    "spark.sql(\"OPTIMIZE delta.`/delta/sales_data`\")\n",
    "\n",
    "# Perform VACUUM\n",
    "spark.sql(\"VACUUM delta.`/delta/sales_data`\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databrics_task3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
