{"cells":[{"cell_type":"markdown","metadata":{"id":"NXNHtF6hacOe"},"source":[" Task 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58122,"status":"ok","timestamp":1727070421787,"user":{"displayName":"Swetha .R","userId":"11283228655930516066"},"user_tz":-330},"id":"WEAA_hI7aOyl","outputId":"7c44de99-f50f-4d77-bf46-197042bcf22a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from Pyspark) (0.10.9.7)\n","Building wheels for collected packages: Pyspark\n","  Building wheel for Pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=84b873268497fbf2a923a2d55db8a58d92df4c736103835b738ed82bddfc2f9a\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built Pyspark\n","Installing collected packages: Pyspark\n","Successfully installed Pyspark-3.5.2\n"]}],"source":["! pip install Pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTO6WViv-Gx8"},"outputs":[],"source":["dbutils.fs.cp(\"file:/Workspace/Shared/customer_transaction.csv\", \"dbfs:/FileStore/streaming/input/customer_transaction.csv\")"]},{"cell_type":"markdown","metadata":{"id":"478nKiB8_E6g"},"source":["create an ETL Pipeline using DLT (Python)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmQLue4tarSG"},"outputs":[],"source":["import dlt\n","from pyspark.sql.functions import col\n","\n","@dlt.table\n","def raw_transactions():\n","\n","    return spark.read.csv(\"dbfs:/FileStore/streaming/input/customer_transaction.csv\", header=True)\n","\n","@dlt.table\n","def transformed_transactions():\n","    df = dlt.read(\"raw_transactions\")\n","    df_transformed = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n","                       .filter(col(\"Quantity\") > 1)\n","\n","    return df_transformed\n"]},{"cell_type":"markdown","metadata":{"id":"OAgfcLpC_JXW"},"source":[" Create an ETL Pipeline using DLT (SQL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ctf-fL9xeAI6"},"outputs":[],"source":["##  Step 1: Define the raw transactions table\n","\n","df_transactions = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/streaming/input/customer_transactions.csv\")\n","\n","df_transactions.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customer_transactions\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc-E22IzAlHh"},"outputs":[],"source":["%sql\n","\n","CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n","SELECT\n","    TransactionID,\n","    TransactionDate,\n","    CustomerID,\n","    Product,\n","    Quantity,\n","    Price,\n","    Quantity * Price AS TotalAmount\n","FROM delta.\"/delta/customer_transactions\";\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nDZP6xuLeX3r"},"source":[" Task 2"]},{"cell_type":"markdown","metadata":{"id":"LenwzyoTCPVo"},"source":[" Delta Lake Operations - Read, Write, Update, Delete, Merge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0z6drhUCW8v"},"outputs":[],"source":["# using python\n","df = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","\n","df.show(5)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-yDNSCheZ1v"},"outputs":[],"source":["%sql\n","SELECT * FROM delta.'/delta/customer_transactions' LIMIT 5;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyxukzUsDEtK"},"outputs":[],"source":["# Create new transactions DataFrame\n","new_data = [\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","new_transactions_df = spark.createDataFrame(new_data, schema=[\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n","\n","\n","new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/delta/customer_transactions\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0loplU7D0C1"},"outputs":[],"source":["## using python\n","\n","from delta.tables import DeltaTable\n","delta_table = DeltaTable.forPath(spark, \"/delta/customer_transactions\")\n","\n","delta_table.update(\n","    condition = \"Product = 'Laptop'\",\n","    set = { \"Price\": \"1300\" }\n",")\n","\n","df_updated = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","df_updated.filter(\"Product = 'Laptop'\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQx2b-RqEDBU"},"outputs":[],"source":["## using sql\n","UPDATE delta.`/delta/customer_transaction`\n","SET Price = 1300\n","WHERE Product = 'Laptop';"]},{"cell_type":"markdown","metadata":{"id":"7msUpPiTERoO"},"source":["Delete Data from Delta Lake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeOttRHtEcJF"},"outputs":[],"source":["## using python\n","delta_table.delete(\"Quantity < 3\")\n","\n","df_after_delete = spark.read.format(\"delta\").load(\"/delta/customer_transactions\")\n","df_after_delete.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJ4I970-E2zQ"},"outputs":[],"source":["## using sql\n","DELETE FROM delta.`/delta/customer_transactions`\n","WHERE Quantity < 3;"]},{"cell_type":"markdown","metadata":{"id":"AdYXlfaiE_7c"},"source":["5. Merge Data into Delta Lake:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18y4ZPDYFGKE"},"outputs":[],"source":["# Create DataFrame for merge\n","merge_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1250),\n","    (8, \"2024-09-08\", \"C007\", \"Charger\", 2, 30)\n","]\n","\n","merge_df = spark.createDataFrame(merge_data, schema=[\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skRuoHVGFXwI"},"outputs":[],"source":["## using python\n","\n","merge_df.createOrReplaceTempView(\"updates\")\n","\n","# Merge statement\n","delta_table.alias(\"target\").merge(\n","    updates.alias(\"source\"),\n","    \"target.TransactionID = source.TransactionID\"\n",").whenMatchedUpdate(\n","    condition=\"target.TransactionID = source.TransactionID\",\n","    set={\"Price\": \"source.Price\", \"Quantity\": \"source.Quantity\", \"TransactionDate\": \"source.TransactionDate\"}\n",").whenNotMatchedInsertAll().execute()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOUaJAfNFtIz"},"outputs":[],"source":["## using sql\n","%sql\n","MERGE INTO delta.`/delta/customer_transactions` AS target\n","USING (SELECT * FROM VALUES\n","    (1, '2024-09-01', 'C001', 'Laptop', 1, 1250),\n","    (8, '2024-09-08', 'C007', 'Charger', 2, 30)\n",") AS source (TransactionID, TransactionDate, CustomerID, Product, Quantity, Price)\n","ON target.TransactionID = source.TransactionID\n","WHEN MATCHED THEN\n","    UPDATE SET target.Price = source.Price, target.Quantity = source.Quantity, target.TransactionDate = source.TransactionDate\n","WHEN NOT MATCHED THEN\n","    INSERT *\n"]},{"cell_type":"markdown","metadata":{"id":"StvyOImrGE9J"},"source":[" Task 3"]},{"cell_type":"markdown","metadata":{"id":"DzwmLiXvGHLT"},"source":["Delta Lake - History, Time Travel, and Vacuum"]},{"cell_type":"markdown","metadata":{"id":"CTqzqySPGPs0"},"source":["1. View Delta Table History:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJZSX_XXGUOG"},"outputs":[],"source":["# Check the transaction history using PySpark\n","delta_table.history().show()\n","# Check file details using sql\n","spark.sql(\"DESCRIBE DETAIL delta.`/delta/customer_transactions`\").show()"]},{"cell_type":"markdown","metadata":{"id":"VtwOJGsFG9yl"},"source":["Perform Time Travel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzwoIgNBHC6l"},"outputs":[],"source":["# Load the table as it was 5 versions ago\n","df_version_5 = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/delta/customer_transactions\")\n","df_version_5.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_I73EtWHSiq"},"outputs":[],"source":["# Retrieve the state of the table at a specific timestamp\n","timestamp = \"2024-09-01T12:00:00\"\n","df_at_time = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp).load(\"/delta/customer_transactions\")\n","\n","df_at_time.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kw3ZeIpSHU9j"},"outputs":[],"source":["## sql\n","%sql\n","SELECT * FROM delta.`/delta/orders` VERSION AS OF 5;\n","\n","%sql\n","SELECT * FROM delta.`/delta/orders` TIMESTAMP AS OF '2024-09-01T12:00:00';\n"]},{"cell_type":"markdown","metadata":{"id":"gRbTF2HaHxQ-"},"source":[" Vacuum the Delta Table:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGj-tJKvIPJG"},"outputs":[],"source":["\n","delta_table.vacuum(retentionHours=168)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uZUOLEHH782"},"outputs":[],"source":["spark.sql(\"VACUUM delta.`/delta/customer_transactions` RETAIN 168 HOURS\")"]},{"cell_type":"markdown","metadata":{"id":"QIWLeKsrIToe"},"source":[" Converting Parquet Files to Delta Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGJ1-Gv3IZ0-"},"outputs":[],"source":["\n","df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/streaming/input/customer_transactions.csv\")\n","\n","df_csv.write.format(\"parquet\").mode(\"overwrite\").save(\"/parquet/customer_transactions\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4OGSsyXIs23"},"outputs":[],"source":["# Convert the Parquet table to Delta\n","spark.read.format(\"parquet\").load(\"/parquet/customer_transactions/\").write.format(\"delta\").mode(\"overwrite\").save(\"/delta/orders_converted\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0ydEnAQI44O"},"outputs":[],"source":["## Convert a Parquet table to Delta using SQL\n","%sql\n","CONVERT TO DELTA parquet.`/parquet/customer_transactions`;\n"]},{"cell_type":"markdown","metadata":{"id":"lGD6eX1iJFQt"},"source":["Task 4"]},{"cell_type":"markdown","metadata":{"id":"CzIfqIxuJHRV"},"source":["Implementing Incremental Load Pattern using Delta Lake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iQTsa8GJM7h"},"outputs":[],"source":["\n","initial_data = [\n","    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1200),\n","    (2, \"2024-09-02\", \"C002\", \"Tablet\", 2, 300),\n","    (3, \"2024-09-03\", \"C001\", \"Headphones\", 5, 50)\n","]\n","\n","columns = [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n","df_initial = spark.createDataFrame(initial_data, columns)\n","\n","df_initial.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/transactions\")\n"]},{"cell_type":"markdown","metadata":{"id":"YZ9BVBhoLA8m"},"source":["Incremental data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbcfCFaSKesm"},"outputs":[],"source":["\n","incremental_data = [\n","    (4, \"2024-09-04\", \"C003\", \"Smartphone\", 1, 800),\n","    (5, \"2024-09-05\", \"C004\", \"Smartwatch\", 3, 200),\n","    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n","]\n","\n","df_incremental = spark.createDataFrame(incremental_data, columns)\n"]},{"cell_type":"markdown","metadata":{"id":"VazkoosBLD3V"},"source":["Implement Incremental Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ITs6lHfLJxr"},"outputs":[],"source":["\n","new_transactions = df_incremental.filter(col(\"TransactionDate\") > \"2024-09-03\")\n","\n","new_transactions.write.format(\"delta\").mode(\"append\").save(\"/delta/transactions\")\n"]},{"cell_type":"markdown","metadata":{"id":"I8g48NPhLPsg"},"source":["Monitor incremental Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qN54r_ONLPMl"},"outputs":[],"source":["\n","%sql\n","DESCRIBE HISTORY delta.`/delta/transactions`\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN2Sakc2w9KnPKUKwF9cZkN","collapsed_sections":["NXNHtF6hacOe","nDZP6xuLeX3r","StvyOImrGE9J","lGD6eX1iJFQt"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
