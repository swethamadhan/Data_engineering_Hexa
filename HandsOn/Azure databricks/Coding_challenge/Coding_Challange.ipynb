{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56419,"status":"ok","timestamp":1727066112121,"user":{"displayName":"Swetha .R","userId":"11283228655930516066"},"user_tz":-330},"id":"G2Pv55sAJKRb","outputId":"8eac9fa4-986f-4f6b-857f-505c716a5f4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=5061c04ca572c7a5253294cf015250c3982039f33e94d6c9e2d949cd0caa82e0\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}],"source":["! pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"in-RNXM8O1e6"},"source":["\n","**Vehicle Maintanace**\n"]},{"cell_type":"markdown","metadata":{"id":"Ta11pEywO9Rg"},"source":["Data Ingestion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmtsqq3WKcP-"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import os\n","spark = SparkSession.builder.appName(\"VehicleMaintenanceDataIngestion\").getOrCreate()\n","file_path = \"/content/sample_data/vehicle_maintenance.csv\"\n","\n","# Check if the file exists\n","if os.path.exists(file_path):\n","    try:\n","        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","        df.show()\n","        df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/vehicle_maintenance\")\n","        print(\"Vehicle maintenance data saved successfully.\")\n","\n","    except Exception as e:\n","        print(f\"Error: {str(e)}\")\n","else:\n","    print(\"CSV file does not exist.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p3-SdTCtPEpH"},"source":["Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-w0FdPl0PHmx"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"VehicleMaintenanceDataCleaning\").getOrCreate()\n","df = spark.read.format(\"delta\").load(\"/delta/vehicle_maintenance\")\n","\n","cleaned_df = df.filter((df.ServiceCost > 0) & (df.Mileage > 0))\n","cleaned_df = cleaned_df.dropDuplicates([\"VehicleID\", \"Date\"])\n","cleaned_df.show()\n","\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_vehicle_maintenance\")\n","print(\"Cleaned data saved successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"7jljqj5OPLMw"},"source":["Vehicle Maintenance Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KM2_v33POdg"},"outputs":[],"source":["\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, col\n","spark = SparkSession.builder.appName(\"VehicleMaintenanceAnalysis\").getOrCreate()\n","\n","df = spark.read.format(\"delta\").load(\"/delta/cleaned_vehicle_maintenance\")\n","\n","total_cost_df = df.groupBy(\"VehicleID\").agg(sum(\"ServiceCost\").alias(\"TotalCost\"))\n","total_cost_df.show()\n","\n","\n","high_mileage_df = df.filter(col(\"Mileage\") > 30000)\n","high_mileage_df.show()\n","\n","total_cost_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/vehicle_total_cost\")\n","high_mileage_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/high_mileage_vehicles\")\n","print(\"Analysis results saved successfully.\")\n"]},{"cell_type":"markdown","metadata":{"id":"vOhCheOhPfBb"},"source":["Data Governance with Delta Lake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mj5rh9PQPitj"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"DeltaLakeGovernance\").getOrCreate()\n","df = spark.read.format(\"delta\").load(\"/delta/cleaned_vehicle_maintenance\")\n","\n","spark.sql(\"VACUUM '/delta/cleaned_vehicle_maintenance' RETAIN 168 HOURS\")\n","print(\"VACUUM operation completed.\")\n","\n","history_df = spark.sql(\"DESCRIBE HISTORY '/delta/cleaned_vehicle_maintenance'\")\n","history_df.show()\n"]},{"cell_type":"markdown","metadata":{"id":"PozwEtCQPvuQ"},"source":["**Movie rating**"]},{"cell_type":"markdown","metadata":{"id":"1-6BmqO2P3XK"},"source":["Data Ingestion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zx2_y5-AP2o9"},"outputs":[],"source":["\n","from pyspark.sql import SparkSession\n","import os\n","\n","spark = SparkSession.builder.appName(\"MovieRatingsDataIngestion\").getOrCreate()\n","file_path = \"/content/sample_data/movie_ratings.csv\"\n","if os.path.exists(file_path):\n","    try:\n","        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","        df.show()\n","        df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/movie_ratings\")\n","        print(\"Movie ratings data saved successfully to Delta table\")\n","\n","    except Exception as e:\n","        print(f\"Error: {str(e)}\")\n","else:\n","    print(f\"CSV file does not exist at {file_path}.\")\n"]},{"cell_type":"markdown","metadata":{"id":"GnBzD44CRPIr"},"source":["Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQRkHyyERR5L"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"MovieRatingsDataCleaning\").getOrCreate()\n","df = spark.read.format(\"delta\").load(\"/delta/movie_ratings\")\n","\n","\n","cleaned_df = df.filter((df.Rating >= 1) & (df.Rating <= 5))\n","cleaned_df = cleaned_df.dropDuplicates([\"UserID\", \"MovieID\"])\n","cleaned_df.show()\n","\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_movie_ratings\")\n","print(\"Cleaned data saved successfully.\")\n"]},{"cell_type":"markdown","metadata":{"id":"po3aKk1-RdFR"},"source":["Movie Rating Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3GNW-XwRhFC"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import avg, col\n","spark = SparkSession.builder.appName(\"MovieRatingsAnalysis\").getOrCreate()\n","df = spark.read.format(\"delta\").load(\"/delta/cleaned_movie_ratings\")\n","\n","\n","avg_rating_df = df.groupBy(\"MovieID\").agg(avg(\"Rating\").alias(\"AvgRating\"))\n","avg_rating_df.show()\n","\n","\n","max_rating_df = avg_rating_df.orderBy(col(\"AvgRating\").desc()).limit(1)\n","min_rating_df = avg_rating_df.orderBy(col(\"AvgRating\").asc()).limit(1)\n","max_rating_df.show()\n","min_rating_df.show()\n","\n","\n","avg_rating_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/movie_avg_ratings\")\n","print(\"Average ratings saved successfully.\")\n"]},{"cell_type":"markdown","metadata":{"id":"MvpYmeN_SJqu"},"source":["Time Travel and Delta Lake History"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGwpQimuSOci"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"MovieRatingsTimeTravel\").getOrCreate()\n","df = spark.read.format(\"delta\").load(\"/delta/cleaned_movie_ratings\")\n","\n","df = df.withColumn(\"Rating\",\n","                   when(df.UserID == 'U001', 5).otherwise(df.Rating))\n","df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_movie_ratings\")\n","\n","\n","original_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/cleaned_movie_ratings\")\n","original_df.show()\n","\n","\n","history_df = spark.sql(\"DESCRIBE HISTORY '/delta/cleaned_movie_ratings'\")\n","history_df.show()\n"]},{"cell_type":"markdown","metadata":{"id":"bqU6FkNtSXXO"},"source":["Optimize Delta Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdDme3y6Saqf"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"OptimizeDeltaTable\").getOrCreate()\n","\n","\n","spark.sql(\"OPTIMIZE '/delta/cleaned_movie_ratings' ZORDER BY (MovieID)\")\n","\n","spark.sql(\"OPTIMIZE '/delta/cleaned_movie_ratings'\")\n","\n","spark.sql(\"VACUUM '/delta/cleaned_movie_ratings' RETAIN 168 HOURS\")\n"]},{"cell_type":"markdown","metadata":{"id":"0bHg3D-sSm1M"},"source":["**Student data**"]},{"cell_type":"markdown","metadata":{"id":"3agaAfegU-Kz"},"source":["Reading Data from Various Formats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnsJdxZDU-q-"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n","\n","csv_file_path = \"/content/Sample_data/student_info.csv\"\n","try:\n","    students_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n","    students_df.show()\n","except Exception as e:\n","    print(f\"Error loading CSV file: {str(e)}\")\n","\n","\n","json_file_path = \"/content/Sample_data/city_info.json\"\n","try:\n","    city_df = spark.read.format(\"json\").load(json_file_path)\n","    city_df.show()\n","except Exception as e:\n","    print(f\"Error loading JSON file: {str(e)}\")\n","\n","\n","parquet_file_path = \"/content/Sample_data/hospitals.parquet\"\n","try:\n","    hospitals_df = spark.read.format(\"parquet\").load(parquet_file_path)\n","    hospitals_df.show()\n","except Exception as e:\n","    print(f\"Error loading Parquet file: {str(e)}\")\n","\n","\n","delta_table_path = \"/delta/hospital_records\"\n","try:\n","    hospital_delta_df = spark.read.format(\"delta\").load(delta_table_path)\n","    hospital_delta_df.show()\n","except Exception as e:\n","    print(f\"Error loading Delta table: {str(e)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"4SGQOSK8XLPA"},"source":["Writing Data to Various Formats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccfl3n0ZXQky"},"outputs":[],"source":["\n","students_df.write.format(\"csv\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/student_info.csv\")\n","\n","city_df.write.format(\"json\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/city_info.json\")\n","\n","\n","hospitals_df.write.format(\"parquet\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/hospital_data.parquet\")\n","\n","hospitals_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/hospital_data\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdxS8i5HXYk7"},"outputs":[],"source":["csv_file_path = \"/content/Sample_data/student_info.csv\"\n","students_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n","\n","\n","cleaned_students_df = students_df.dropDuplicates().na.fill({'Score': 0})\n","cleaned_students_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_students\")\n","\n","dbutils.notebook.run(\"/path/to/Notebook_B\", 60)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03xN1A3-X04Y"},"outputs":[],"source":["\n","cleaned_students_df = spark.read.format(\"delta\").load(\"/delta/cleaned_students\")\n","\n","\n","avg_score_df = cleaned_students_df.groupBy(\"Class\").avg(\"Score\")\n","avg_score_df.show()\n","\n","avg_score_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/average_student_scores\")\n"]},{"cell_type":"markdown","metadata":{"id":"XvA84BiGX9fU"},"source":["Databricks Ingestion from Various Sources"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"At6b78JYYBxx"},"outputs":[],"source":["\n","spark = SparkSession.builder.appName(\"DatabricksIngestion\").getOrCreate()\n","\n","#  Reading CSV from Azure Data Lake\n","csv_file_path_adl = \"abfss://<container>@<storage_account>.dfs.core.windows.net/student_info.csv\"\n","adl_students_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path_adl)\n","adl_students_df.show()\n","\n","# Reading JSON from Databricks FileStore\n","json_file_path = \"/dbfs/FileStore/city_info.json\"\n","filestore_city_df = spark.read.format(\"json\").load(json_file_path)\n","filestore_city_df.show()\n","\n","# Reading Parquet from AWS S3\n","parquet_file_path_s3 = \"s3a://<bucket>/hospital_data.parquet\"\n","s3_hospital_df = spark.read.format(\"parquet\").load(parquet_file_path_s3)\n","s3_hospital_df.show()\n","\n","#  Delta Table stored in Databricks\n","delta_table_path = \"/delta/hospital_records\"\n","delta_hospital_df = spark.read.format(\"delta\").load(delta_table_path)\n","delta_hospital_df.show()\n","\n","# Performing transformations:\n","filtered_students_df = adl_students_df.filter(adl_students_df.Score > 80)\n","filtered_students_df.show()\n","\n","# Writing cleaned data to CSV, JSON, Parquet, and Delta formats\n","filtered_students_df.write.format(\"csv\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/filtered_students.csv\")\n","filestore_city_df.write.format(\"json\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/filtered_city_data.json\")\n","s3_hospital_df.write.format(\"parquet\").mode(\"overwrite\").save(\"/dbfs/FileStore/output/filtered_hospitals.parquet\")\n","delta_hospital_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/filtered_hospital_data\")\n"]},{"cell_type":"markdown","metadata":{"id":"qbSbKMtXYc6p"},"source":["Aditional Task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CT-SNhB7YYRI"},"outputs":[],"source":["\n","spark.sql(\"OPTIMIZE '/delta/filtered_hospital_data'\")\n","\n","spark.sql(\"OPTIMIZE '/delta/filtered_hospital_data' ZORDER BY (CityName)\")\n","\n","spark.sql(\"VACUUM '/delta/filtered_hospital_data' RETAIN 168 HOURS\")\n","\n","\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5P2sVz18+BU9MGZEowV7W","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
