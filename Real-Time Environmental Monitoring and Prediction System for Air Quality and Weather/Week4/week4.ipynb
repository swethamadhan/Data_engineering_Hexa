{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a050319-5b13-4b6b-95a1-485f49b82719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/environment_data.csv\",\"dbfs/FileStore/environment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b420facd-2203-42e2-a7eb-aafb47a31756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n|            features|PM25|        prediction|\n+--------------------+----+------------------+\n|[15.2,20.1,410.0,...|15.2|15.200000000000003|\n+--------------------+----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EnvironmentalPrediction\").getOrCreate()\n",
    "\n",
    "# Load the environmental data\n",
    "env_df = spark.read.csv(\"/dbfs/FileStore/environment_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Feature engineering: prepare features for the model\n",
    "assembler = VectorAssembler(inputCols=[ 'PM25', 'PM10', 'CO2', 'Temperature', 'Humidity', 'WindSpeed'], outputCol=\"features\", handleInvalid=\"skip\")\n",
    "env_df = assembler.transform(env_df)\n",
    "\n",
    "train_data, test_data = env_df.randomSplit([0.8, 0.2])\n",
    "# Train a decision tree model to predict air quality levels\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PM25\")\n",
    "model = dt.fit(env_df)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"PM25\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830bca7a-8fd0-46a7-bc11-ce0de31208d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"/dbfs/FileStore/environment_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0298d8-b4ae-4223-b0ad-b6879c9f1094",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n|            features|prediction|\n+--------------------+----------+\n|[40.1,55.0,420.0,...|      23.5|\n|[42.3,52.5,425.0,...|      23.5|\n|[43.0,56.0,430.0,...|      23.5|\n|[45.2,60.0,440.0,...|      23.5|\n|[46.5,61.5,445.0,...|      23.5|\n|[48.0,63.0,450.0,...|      23.5|\n|[49.5,65.5,455.0,...|      23.5|\n|[50.0,67.0,460.0,...|      23.5|\n+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressionModel\n",
    "\n",
    "loaded_model = DecisionTreeRegressionModel.load(\"/dbfs/FileStore/environmental_model\")\n",
    "\n",
    "new_data = spark.read.csv(\"dbfs:/FileStore/new_environment_data.csv\", header=True, inferSchema=True)\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "future_predictions = loaded_model.transform(new_data)\n",
    "future_predictions.select(\"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1277a519-a024-4fee-ba22-623d8339fa91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "# Define the schema for the CSV data\n",
    "schema = StructType([\n",
    "    StructField(\"DateTime\", StringType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"CO2\", DoubleType(), True),\n",
    "    StructField(\"Temperature\", DoubleType(), True),\n",
    "    StructField(\"Humidity\", DoubleType(), True),\n",
    "    StructField(\"WindSpeed\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "streaming_df = (spark\n",
    "    .readStream\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)  # Use the defined schema\n",
    "    .csv(\"dbfs:/FileStore/\")\n",
    "  ) \n",
    "processed_stream = streaming_df.select(\"DateTime\", \"PM25\", \"Temperature\", \"Humidity\", \"WindSpeed\")\n",
    "\n",
    "query = (processed_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")  \n",
    "    .start())\n",
    "\n",
    "anomalies = streaming_df.filter(\n",
    "    (col(\"PM25\") > 45) | (col(\"Temperature\") > 35)\n",
    ")\n",
    "\n",
    "anomalies.show()\n",
    "# query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Environment_data_DB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
