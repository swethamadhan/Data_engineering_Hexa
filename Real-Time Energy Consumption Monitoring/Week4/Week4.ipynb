{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fabeb7-d4d6-4226-b32b-f5638073ce1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/energy_consumption_data.csv\",\"dbfs/FileStore/energy_consumption_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9329b15b-491d-4ac1-b629-6bb1262e466f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-----------+-----------+---------------+\n|       timestamp|temperature|time_of_day|device_type|energy_consumed|\n+----------------+-----------+-----------+-----------+---------------+\n|10-09-2024 08:00|       22.5|          8|      meter|            100|\n|10-09-2024 09:00|       23.0|          9|     sensor|            110|\n|10-09-2024 10:00|       24.1|         10|      meter|            120|\n|10-09-2024 11:00|       25.3|         11|     sensor|            115|\n|10-09-2024 12:00|       26.4|         12|  appliance|            130|\n|10-09-2024 13:00|       27.0|         13|      meter|            125|\n+----------------+-----------+-----------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EnergyConsumptionPrediction\").getOrCreate()\n",
    "\n",
    "# Load the data from a CSV file\n",
    "df = spark.read.csv(\"/dbfs/FileStore/energy_consumption_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preview the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a076272e-62e4-44de-94e5-194540503236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------------------+\n|       features|energy_consumed|        prediction|\n+---------------+---------------+------------------+\n|[25.3,11.0,2.0]|            115|123.41059602649246|\n+---------------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Index the 'device_type' column (if it's categorical)\n",
    "device_type_indexer = StringIndexer(inputCol=\"device_type\", outputCol=\"device_type_index\")\n",
    "\n",
    "# Assemble feature columns\n",
    "feature_columns = ['temperature', 'time_of_day', 'device_type_index']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"energy_consumed\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[device_type_indexer, assembler, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Fit the model on the training data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Test the model on the test data\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "predictions.select(\"features\", \"energy_consumed\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64abaf65-d733-4f49-adec-3c899a3e09d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 8.410596026492456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_consumed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc89d038-ba25-4898-a3c9-ba195b8e8141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: -inf\n"
     ]
    }
   ],
   "source": [
    "# Print R-squared value\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"energy_consumed\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ef888f-0a93-4acf-9e6b-e8e097d707b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/future_energy_data.csv\",\"dbfs/FileStore/future_energy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80acfcbd-d2d8-498c-9cdc-f91a1d6e9036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+------------------+\n|       timestamp|       features|        prediction|\n+----------------+---------------+------------------+\n|11-09-2024 08:00| [24.3,8.0,0.0]| 109.3832781457157|\n|11-09-2024 09:00| [25.7,9.0,2.0]|120.17384105962915|\n|11-09-2024 10:00|[26.5,10.0,1.0]|123.09188741724259|\n|11-09-2024 11:00|[27.8,11.0,0.0]|127.64486754969568|\n|11-09-2024 12:00|[29.0,12.0,2.0]|137.78145695367326|\n|11-09-2024 13:00|[28.4,13.0,1.0]| 136.1216887417357|\n|11-09-2024 14:00|[22.7,14.0,0.0]|117.78559602643394|\n|11-09-2024 15:00|[23.6,15.0,2.0]|126.94122516550773|\n|11-09-2024 16:00|[25.0,16.0,1.0]|131.82119205292872|\n|11-09-2024 17:00|[27.1,17.0,0.0]|138.99006622512522|\n|11-09-2024 18:00|[24.9,18.0,2.0]| 138.0091059601933|\n|11-09-2024 19:00|[28.3,19.0,1.0]|149.42880794697282|\n|11-09-2024 20:00|[26.7,20.0,0.0]|144.49917218535603|\n|11-09-2024 21:00|[23.4,21.0,2.0]| 139.9213576157769|\n|11-09-2024 22:00|[24.5,22.0,1.0]|143.82036423829413|\n|11-09-2024 23:00|[29.2,23.0,0.0]|159.49089403965672|\n|12-09-2024 00:00| [21.9,0.0,2.0]| 87.29718543054037|\n|12-09-2024 01:00| [25.8,1.0,1.0]|100.35182119215953|\n|12-09-2024 02:00| [27.6,2.0,0.0]|106.53973509945226|\n|12-09-2024 03:00| [22.8,3.0,2.0]| 97.05711920535424|\n+----------------+---------------+------------------+\n\nPredictions saved to CSV file at /dbfs/FileStore/future_energy_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Load new data for prediction (future data)\n",
    "new_data = spark.read.csv(\"/dbfs/FileStore/future_energy_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Transform the new data using the same pipeline\n",
    "new_data_transformed = pipeline_model.transform(new_data)\n",
    "\n",
    "# Apply the pre-trained model to the future data\n",
    "predictions = pipeline_model.transform(new_data)\n",
    "\n",
    "# Display the predictions\n",
    "predictions.select(\"timestamp\", \"features\", \"prediction\").show()\n",
    "\n",
    "# Save the predictions to a new CSV file\n",
    "predictions.select(\"timestamp\", \"prediction\").write.csv(\"/dbfs/FileStore/future_energy_predictions.csv\", header=True)\n",
    "\n",
    "print(\"Predictions saved to CSV file at /dbfs/FileStore/future_energy_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c082cff2-0c09-4053-a980-2338c6aee6c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|       timestamp|        prediction|\n+----------------+------------------+\n|11-09-2024 08:00| 109.3832781457157|\n|11-09-2024 09:00|120.17384105962915|\n|11-09-2024 10:00|123.09188741724259|\n|11-09-2024 11:00|127.64486754969568|\n|11-09-2024 12:00|137.78145695367326|\n|11-09-2024 13:00| 136.1216887417357|\n|11-09-2024 14:00|117.78559602643394|\n|11-09-2024 15:00|126.94122516550773|\n|11-09-2024 16:00|131.82119205292872|\n|11-09-2024 17:00|138.99006622512522|\n|11-09-2024 18:00| 138.0091059601933|\n|11-09-2024 19:00|149.42880794697282|\n|11-09-2024 20:00|144.49917218535603|\n|11-09-2024 21:00| 139.9213576157769|\n|11-09-2024 22:00|143.82036423829413|\n|11-09-2024 23:00|159.49089403965672|\n|12-09-2024 00:00| 87.29718543054037|\n|12-09-2024 01:00|100.35182119215953|\n|12-09-2024 02:00|106.53973509945226|\n|12-09-2024 03:00| 97.05711920535424|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the predictions CSV file\n",
    "predicted_df = spark.read.csv(\"/dbfs/FileStore/future_energy_predictions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the predictions\n",
    "predicted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb44cb02-a010-446f-bb56-51a12442ed5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"time_of_day\", IntegerType(), True),\n",
    "    StructField(\"device_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "streaming_df = (spark\n",
    "    .readStream\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)  # Use the defined schema\n",
    "    .csv(\"/dbfs/FileStore/future_energy_predictions.csv\")) \n",
    "\n",
    "processed_stream = streaming_df.select(\"timestamp\", \"temperature\", \"time_of_day\", \"device_type\")\n",
    "\n",
    "query = (processed_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start())\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc449599-7a68-454d-8c60-2ab40cd031f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_query = (processed_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"csv\")  # Write to CSV\n",
    "    .option(\"path\", \"/dbfs/FileStore/\")\n",
    "    .option(\"checkpointLocation\", \"/dbfs/FileStore/\")\n",
    "    .start())\n",
    "\n",
    "# output_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Energy_Consumption_DB_Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
